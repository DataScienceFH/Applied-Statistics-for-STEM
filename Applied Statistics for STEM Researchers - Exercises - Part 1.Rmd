---
title: "Applied Statistics for STEM Researchers - Exercises - Part 1"
author: "Frank Hause"
date: "`r Sys.Date()`"
output: html_document
---

# Task 1: Data Types

```{r}
# ──────────────────────────────────────────────
# Task 1 — Interpreting Types of Data
# ──────────────────────────────────────────────
# Decide for each vector whether it represents:
#   • categorical / nominal data
#   • ordinal data
#   • metric data (discrete or continuous)
# and explain your reasoning briefly.
# ──────────────────────────────────────────────

vector_of_integers     <- c(120L, 132L, 98L, 110L, 145L, 121L)
vector_of_denominators <- c(120L, 132L, 98L, 110L, 145L, 121L)
vector_of_floats       <- c(120L, 132L, 98L, 110L, 145L, 121L)
vector_of_factors      <- c(120L, 132L, 98L, 110L, 145L, 121L)
vector_of_vectors      <- c(vector_of_integers, vector_of_integers, vector_of_integers)

# Note:
# The numerical form alone does not determine the statistical type —
# interpretation depends on the intended meaning of the data.
```

# Task 2: Distributions

```{r}
# ──────────────────────────────────────────────
# Task 2 — Assessing Distributions Visually
# ──────────────────────────────────────────────
# 1. Which of the following distributions approximates a Normal
#    distribution second best and which worst?
# 2. For each distribution, provide a biological or experimental
#    example where it may occur.
# ──────────────────────────────────────────────

set.seed(1)

# Generate data from various distributions
values_normal   <- stats::rnorm(200, mean = 0, sd = 1)           # Gaussian / Normal
values_cauchy   <- stats::rcauchy(200, location = 0, scale = 1)  # heavy tails
values_poisson  <- stats::rpois(200, lambda = 3)                 # discrete counts
values_uniform  <- stats::runif(200, min = -2, max = 2)          # equal (uniform)
values_bimodal  <- c(stats::rnorm(100, -2, 0.5),
                     stats::rnorm(100,  2, 0.5))                 # two peaks
values_skewed   <- stats::rexp(200, rate = 1) - 1                # right-skewed exponential

# Visual comparison
graphics::par(mfrow = c(2, 3))
graphics::hist(values_normal,   main = "Normal",      col = "grey", xlab = "")
graphics::hist(values_cauchy,   main = "Cauchy",      col = "grey", xlab = "")
graphics::hist(values_poisson,  main = "Poisson",     col = "grey", xlab = "")
graphics::hist(values_uniform,  main = "Uniform",     col = "grey", xlab = "")
graphics::hist(values_bimodal,  main = "Bimodal",     col = "grey", xlab = "")
graphics::hist(values_skewed,   main = "Exponential", col = "grey", xlab = "")
graphics::par(mfrow = c(1, 1))

# Shapiro–Wilk tests
tests <- c(
  Normal   = stats::shapiro.test(values_normal)$p.value,
  Cauchy   = stats::shapiro.test(values_cauchy)$p.value,
  Poisson  = stats::shapiro.test(values_poisson)$p.value,
  Uniform  = stats::shapiro.test(values_uniform)$p.value,
  Bimodal  = stats::shapiro.test(values_bimodal)$p.value,
  Exp      = stats::shapiro.test(values_skewed)$p.value
)
tests

```

# Task 3: Standard Normal Distribution

```{r}
# ──────────────────────────────────────────────
# Task 3 — Standard Normal Distribution (Z)
# ──────────────────────────────────────────────
# 1. Plot the standard normal density.
# 2. Add vertical dashed lines at integer Z values.
# 3. Estimate at which Z values the right-tail area equals:
#       p = 0.5, 0.1, 0.05, and 0.01
# ──────────────────────────────────────────────

# Sequence of Z values
z <- seq(-4, 4, by = 0.01)

# Compute density
density_z <- stats::dnorm(z, mean = 0, sd = 1)

# Plot the bell curve
graphics::plot(
  z, density_z, type = "l", lwd = 2, col = "black",
  main = "Standard Normal Distribution (Z)",
  xlab = "Z value", ylab = "Density"
)

# Add vertical dashed lines for key Z values
z_lines <- c(-3, -2, -1, 0, 1, 2, 3)
graphics::abline(v = z_lines, lty = 2, col = "grey40")

# Example: right-tail probability for Z = 0.8
1 - stats::pnorm(0.8)  # ≈ 0.2118

```

# Task 4: Simpson's Paradox

```{r}
# ──────────────────────────────────────────────
# Task 4 — Simpson’s Paradox: Reversal through Aggregation
# ──────────────────────────────────────────────
# Background:
# Simpson’s paradox occurs when the direction of an association
# reverses after combining data across a confounding variable.
# Here, both subgroups (small and large stones) show higher
# success for Treatment A, yet the combined data suggests
# Treatment B is better.
#
# 1. Compare success rates of Treatment A and B within each stone size.
# 2. Combine the groups and test again.
# 3. Explain why the effect direction reverses in the pooled data.
# ──────────────────────────────────────────────

# Construct contingency tables (simplified kidney stone example)
# Rows = outcome (Success/Failure), Columns = Treatment (A/B)

# Small stones
small <- matrix(c(
  81, 234,  # Successes: A, B
  6,  36    # Failures:  A, B
), nrow = 2, byrow = TRUE,
dimnames = list(Outcome = c("Success","Failure"),
                Treatment = c("A","B")))

# Large stones
large <- matrix(c(
  192, 55,  # Successes: A, B
  71, 25    # Failures:  A, B
), nrow = 2, byrow = TRUE,
dimnames = list(Outcome = c("Success","Failure"),
                Treatment = c("A","B")))

# Combine totals across both stone sizes
total <- small + large

# Display tables
small
large
total

# Run Fisher’s exact tests
stats::fisher.test(small)   # Small stones
stats::fisher.test(large)   # Large stones
stats::fisher.test(total)   # Combined

# Calculate success proportions
prop.table(small, 2)   # within treatment for small stones
prop.table(large, 2)   # within treatment for large stones
prop.table(total, 2)   # within treatment for total cohort

```

# Task 5: Outlier Sensitivity in Two-Group Comparisons

```{r}
# ──────────────────────────────────────────────
# Task 5 — Outlier Robustness in Two-Group Comparisons
# ──────────────────────────────────────────────
# 1. Compare t-test and Wilcoxon rank-sum on clean data.
# 2. Replace one value with a single extreme outlier and compare again.
# 3. Append additional outliers (increase their count) and observe how
#    both tests degrade.
# 4. Decide which test is more stable to outliers and explain why.
# ──────────────────────────────────────────────

set.seed(2)
A <- stats::rnorm(20, mean = 0,  sd = 1)
B <- stats::rnorm(20, mean = 1,  sd = 1)

# 1) Clean data: baseline comparison
p_t_clean  <- stats::t.test(A, B)$p.value   # t.test
p_w_clean  <- stats::wilcox.test(A, B)$p.value                 # Rank-sum test
p_t_clean
p_w_clean

# 2) Contamination by replacement: one extreme outlier in B
B_out <- B
B_out[1] <- 17.5

p_t_repl  <- stats::t.test(A, B_out)$p.value
p_w_repl  <- stats::wilcox.test(A, B_out)$p.value
p_t_repl
p_w_repl

# Helpers:
# Contamination by appending: add k extreme outliers to B
append_outliers <- function(A, B, k = 1, value = 17.5) {
  B_app <- c(B, rep(value, k))
  A_app <- c(A, rep(0,    k))   # keep A length comparable (neutral padding)
  c(
    p_t = stats::t.test(A_app, B_app, var.equal = FALSE)$p.value,
    p_w = stats::wilcox.test(A_app, B_app)$p.value
  )
}

# Evaluate growing number of appended outliers
k_values <- c(1, 2, 5, 10)
p_mat <- sapply(k_values, function(k) append_outliers(A, B, k = k, value = 17.5))
colnames(p_mat) <- paste0("k=", k_values)
p_mat

# Quick printout of baseline vs 1 replacement
c(
  t_clean  = p_t_clean,
  w_clean  = p_w_clean,
  t_repl1  = p_t_repl,
  w_repl1  = p_w_repl
)

# Hint: You can append values by:
# A_new <- c(A, 170)
# B_new <- c(B, 170)

```

# Task 6: Ill-constructed ANOVA

```{r}
# ──────────────────────────────────────────────
# Task 6 — Plants in Chambers: Misleading ANOVA
# ──────────────────────────────────────────────
# Background:
# A biologist investigates whether light color affects
# plant growth. She grows ten plants per color in separate
# chambers (one chamber per light color), then performs
# an ANOVA on individual plant heights.
#
# 1. Interpret the result of the initial ANOVA.
# 2. Explain why this experimental setup is ill-constructed.
# 3. Suggest how to design the experiment properly.
# 4. Describe or perform the correct ANOVA.
# ──────────────────────────────────────────────

set.seed(42)

# Simulated plant heights (cm)
red   <- stats::rnorm(10, mean = 18, sd = 1.5)   # Chamber 1
blue  <- stats::rnorm(10, mean = 22, sd = 1.5)   # Chamber 2
white <- stats::rnorm(10, mean = 26, sd = 1.5)   # Chamber 3

# Combine into one dataset
growth <- c(red, blue, white)
light  <- factor(rep(c("Red", "Blue", "White"), each = 10))

# Perform the ANOVA
anova_wrong <- stats::aov(growth ~ light)
summary(anova_wrong)

```

```{r}
# ──────────────────────────────────────────────
# Task 6 (continued) — Correct ANOVA with Proper Design
# ──────────────────────────────────────────────
# Proper experimental design:
# | Light color | Number of chambers | Plants per chamber |
# |--------------|--------------------|--------------------|
# | Red          | 3                  | 10                 |
# | Blue         | 3                  | 10                 |
# | White        | 3                  | 10                 |
#
# 1. Simulate a properly replicated experiment.
# 2. Compute chamber-level means.
# 3. Perform an ANOVA at the chamber level.
# 4. Interpret why this design avoids pseudoreplication.
# ──────────────────────────────────────────────

set.seed(42)

# 3 chambers per light color, 10 plants each
growth  <- numeric()
light   <- character()
chamber <- character()

means <- c(Red = 18, Blue = 22, White = 26)

for (col in names(means)) {
  for (rep in 1:3) {  # three chambers per light color
    # Each chamber has a slightly different mean due to random variation
    chamber_mean <- stats::rnorm(1, mean = means[col], sd = 1.0)
    # Within-chamber variation among plants
    plant_values <- stats::rnorm(10, mean = chamber_mean, sd = 1.5)
    
    growth   <- c(growth, plant_values)
    light    <- c(light, rep(col, 10))
    chamber  <- c(chamber, rep(paste0(col, "_", rep), 10))
  }
}

df <- data.frame(growth, light = factor(light), chamber = factor(chamber))

# Aggregate chamber means (chambers = independent replicates)
chamber_means <- aggregate(growth ~ light + chamber, data = df, FUN = mean)

# Correct ANOVA based on chamber means
anova_correct <- stats::aov(growth ~ light, data = chamber_means)
summary(anova_correct)

```
